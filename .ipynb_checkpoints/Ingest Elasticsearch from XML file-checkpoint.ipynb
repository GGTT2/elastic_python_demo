{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ingest data in Elasticsearch from an XML file\n",
    "\n",
    "Welcome in this notebook. Its purpose is to show how to ingest large XML data to Elasticsearch and Kibana. \n",
    "\n",
    "\n",
    "In the following lines, we will \n",
    "* start by downloading a XML file from an online API (in this example : Isidore)\n",
    "* stream through the file (necessary for big files)\n",
    "* use python client to index the document to Elasticsearch\n",
    "\n",
    "Notice that for the sake of the example, the XML file is very small, however this method is designed to suit large XML file. \n",
    "\n",
    "### Prerequisite\n",
    "\n",
    "* Have both Elasticsearch and Kibana running. \n",
    "* Install the requirements.txt \n",
    "`pip install -r requirements.txt`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading the file from online API\n",
    "\n",
    "In this example, we are going to work with an XML file. \n",
    "Either follow on the process from the API or go streight to the definition of the file (FILE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from xml.etree import ElementTree\n",
    "import lxml.etree as ET #this library manages xpath better\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieving data from an API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://api.isidore.science/resource/search\" # ardress of isidore API\n",
    "subject = \"http://data.bnf.fr/ark:/12148/cb16620091k\" # in this example we are looking for \"digital humanities\" subject\n",
    "nb_result = 50 # in this example, we are only looking for 50 results\n",
    "\n",
    "response = requests.get(url, {\"subject\": subject, \"replies\": nb_result} )\n",
    "data = ElementTree.fromstring(response.content)\n",
    "tree = ElementTree.ElementTree(data) #this step creates a manageable XML tree, rather than a text. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Printing data in a file \n",
    "\n",
    "If you are dealing with small datasets, this step is not compulsory ; you can store the data in your memory and work directly from the `response` variable. \n",
    "In this example, to purpose is to show how to load big xml file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE = \"export.xml\" # here we are loading the data to a new file \"export.xml\"\n",
    "\n",
    "\n",
    "tree.write(open(FILE, 'w'), encoding=\"unicode\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading document by document the XML file\n",
    "\n",
    "In this example, the xml file is not a very large one. It contains 50 documents to index, which is not a lot!\n",
    "However, let's pretend you are willing to manipulate a very large XML file! In order not to run out of memory, it is better to read it entry by entry. This is the next step!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the variables\n",
    "\n",
    "In order to parse the XML document we need to be aware of it structure. Have a look at your XML file, and determine which is the tag for each entry you want to index in Elasticsearch. \n",
    "Change de following NODE variable according to your document, or keep it to run this exemple. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NODE = \"reply\" #in our case, we can either chose \"entry\" or \"isidore\" (each entry has only one direct child which is isidore tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relevant_nodes(xml_data):\n",
    "    my_dict = {}\n",
    "    my_dict['_id'] = unicodedata.normalize('NFKD', xml_data.xpath(\"./@uri\")[0])\n",
    "    my_dict['authors'] = [unicodedata.normalize('NFKD', d) for d in xml_data.xpath(\"//creator/@normalizedAuthor\")]\n",
    "    my_dict['date'] = unicodedata.normalize('NFKD', next(iter(xml_data.xpath(\"//date[1]/@origin\")),\"\")) #this syntax returns first element [0] or \"\"\n",
    "    my_dict['subjects'] = [unicodedata.normalize('NFKD', d) for d in xml_data.xpath(\"//subject/text()\")]\n",
    "    my_dict['abstract'] = unicodedata.normalize('NFKD', next(iter(xml_data.xpath(\"//abstract/text()\")), \"\"))\n",
    "    my_dict['concepts'] = [unicodedata.normalize('NFKD', d) for d in xml_data.xpath(\"//concept/preflabel[@xml:lang='fr']/text()\")]\n",
    "    for k in my_dict.keys():\n",
    "        if my_dict[k]:\n",
    "            return my_dict #we only return my_dict if at least one of the value is filled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stream XML\n",
    "\n",
    "This function is designed to read piece by piece the XML document. Each time it recognizes a tag \"NODE\", it manages its content, before erasing it from memory. This method streams throught the XML document and prevent out of memory errors. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def streamXML():\n",
    "    \"\"\"\n",
    "    This function reads the XML file piece by piece, and \"acts\" when it recognize a specific node. \n",
    "    \"\"\"\n",
    "    for event, elem in ET.iterparse(FILE, events=('start', 'end')):\n",
    "        if event == 'end' and elem.tag == NODE : # Each time the XML parser meets the end tag of our \"NODE\" (here \"isidore\")\n",
    "            #print(elem.xpath(\"./@uri\"))\n",
    "            \n",
    "            my_dict = get_relevant_nodes(elem)\n",
    "            elem.clear() #Don't forget to free memory space!\n",
    "\n",
    "            if my_dict:\n",
    "                yield my_dict # this function \"yields\" for each element the specific node as a dictionnary. It will be processed by the index function. \n",
    "            else:\n",
    "                pass   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Index\n",
    "\n",
    "Now, we need to set up the index function!  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch.helpers import streaming_bulk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Set up Elastic client\n",
    "\n",
    "In this example, the index is going to be created automatically.  \n",
    "We still need to set up our Elasticsearch client.  \n",
    "If you need help and example to set up your elastic config [see the documentation](https://elasticsearch-py.readthedocs.io/en/master/#ssl-and-authentication)\n",
    "\n",
    "#### Bulk API\n",
    "\n",
    "Indexing with Elasticsearch, there are 2 mistakes you don't want to make: \n",
    "* index each document 1 by 1 (long and coastly)\n",
    "* index aaaaall documents at once (especially if you have a big number of documents indeed, let's say, more than 500). \n",
    "\n",
    "The solution is to group a reasonable amount of documents (a chunk) and bulk ingest it, before moving to the next chunk. \n",
    "\n",
    "*In this example, we are dealing with a very small number of documents, but it is important to understand this concept for bigger usecase*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ES_CONFIG = [] #in this example, the config can be empty because of a basic configuration (no login, ES on localhost:9200)\n",
    "INDEX = \"isidore\" #change this name accordingly to your dataset. \n",
    "CHUNK = 500 #this is the default value of the library. In this example, we will only have 1 chunk, because we have only 50 entries. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index():\n",
    "    client = Elasticsearch(\n",
    "        #CONFIG\n",
    "    )    \n",
    "    print(\"Indexing documents...\")\n",
    "    successes = 0\n",
    "    for ok, action in streaming_bulk(client=client, chunk_size=CHUNK, index=INDEX, actions=streamXML()): #this calls the function streaming_bulk\n",
    "        successes += ok\n",
    "    print(f'Indexed {successes} documents')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kibana",
   "language": "python",
   "name": "kibana"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
